<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Project 4 ¬∑ Diffusion Models Overview</title>
    <style>
      :root {
        color-scheme: light;
        --bg: #f4f6fb;
        --card: #ffffff;
        --text: #1a1d24;
        --muted: #60697a;
        --accent: #5865f2;
        --accent-light: rgba(88, 101, 242, 0.12);
        --border: rgba(17, 24, 39, 0.08);
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family: "Inter", "Segoe UI", system-ui, -apple-system, sans-serif;
        background: var(--bg);
        color: var(--text);
        line-height: 1.6;
        padding: 32px 16px 80px;
      }

      .shell {
        max-width: 1100px;
        margin: 0 auto;
      }

      header {
        background: linear-gradient(120deg, #1f1c2c, #403b8c, #5865f2);
        color: white;
        border-radius: 28px;
        padding: 48px;
        box-shadow: 0 24px 60px rgba(31, 28, 44, 0.34);
        margin-bottom: 32px;
      }

      header h1 {
        font-size: clamp(2rem, 4vw, 3rem);
        font-weight: 700;
      }

      header p {
        margin-top: 12px;
        color: rgba(255, 255, 255, 0.85);
        font-size: 1.05rem;
      }

      header .tags {
        display: flex;
        flex-wrap: wrap;
        gap: 10px;
        margin-top: 20px;
      }

      header .tags span {
        background: rgba(255, 255, 255, 0.12);
        border: 1px solid rgba(255, 255, 255, 0.3);
        border-radius: 999px;
        padding: 8px 16px;
        font-size: 0.9rem;
        backdrop-filter: blur(6px);
      }

      section {
        background: var(--card);
        border-radius: 22px;
        padding: 32px;
        margin-bottom: 24px;
        border: 1px solid var(--border);
        box-shadow: 0 20px 50px rgba(15, 23, 42, 0.08);
      }

      h2 {
        font-size: 1.35rem;
        margin-bottom: 20px;
      }

      p {
        color: var(--muted);
        font-size: 1rem;
      }

      .grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
        gap: 18px;
      }

      .card {
        background: var(--accent-light);
        border-radius: 20px;
        padding: 22px;
        border: 1px solid rgba(88, 101, 242, 0.2);
      }

      .card strong {
        display: block;
        font-size: 2rem;
        color: var(--accent);
      }

      .timeline {
        display: grid;
        gap: 18px;
      }

      .timeline-item {
        display: grid;
        grid-template-columns: 110px 1fr;
        gap: 16px;
        align-items: baseline;
      }

      .bubble {
        background: #0e1117;
        color: #f7f9fc;
        padding: 8px 12px;
        border-radius: 999px;
        font-size: 0.85rem;
        text-align: center;
      }

      ul {
        list-style: none;
        display: grid;
        gap: 12px;
        margin-top: 12px;
        color: var(--muted);
      }

      ul li {
        padding-left: 26px;
        position: relative;
      }

      ul li::before {
        content: "‚Ä¢";
        color: var(--accent);
        position: absolute;
        left: 8px;
        font-size: 1.6rem;
        line-height: 1;
      }

      .links {
        display: flex;
        flex-wrap: wrap;
        gap: 12px;
        margin-top: 20px;
      }

      .links a {
        text-decoration: none;
        color: white;
        background: var(--text);
        padding: 12px 18px;
        border-radius: 14px;
        font-weight: 500;
        transition: transform 0.2s ease;
      }

      .links a:hover {
        transform: translateY(-2px);
      }

      @media (max-width: 720px) {
        header {
          padding: 32px;
        }
        section {
          padding: 24px;
        }
        .timeline-item {
          grid-template-columns: 1fr;
        }
        .bubble {
          width: fit-content;
        }
      }
    </style>
  </head>
  <body>
    <div class="shell">
      <header>
        <h1>Project 4 ¬∑ Diffusion Models</h1>
        <p>
          Hands-on exploration of forward diffusion, classical vs. learned
          denoising, classifier-free guidance, and creative image-to-image
          editing with DeepFloyd IF. All experiments share the same random seed
          (<code>180</code>) to keep outputs traceable.
        </p>
        <div class="tags">
          <span>PyTorch ¬∑ Diffusers</span>
          <span>DeepFloyd IF</span>
          <span>Gaussian & CFG Denoising</span>
          <span>Text-to-Image + SDEdit</span>
        </div>
      </header>

      <section>
        <h2>Project Snapshot</h2>
        <div class="grid">
          <div class="card">
            <strong>3</strong>
            base prompts sampled in Part‚ÄØ0, each rendered at 64‚ÄØ‚Üí‚ÄØ256 px using
            two inference step counts.
          </div>
          <div class="card">
            <strong>5</strong>
            CFG-guided generations for ‚Äúa high quality photo‚Äù (Œ≥‚ÄØ=‚ÄØ7) to compare
            conditional vs. unconditional noise.
          </div>
          <div class="card">
            <strong>6</strong>
            noise levels revisited during SDEdit (1‚Ä¶20) to reveal how edits fade
            back to the original image.
          </div>
          <div class="card">
            <strong>2</strong>
            custom photos (campanile, sea scene) reused across forward diffusion,
            iterative denoising, and editing pipelines.
          </div>
        </div>
      </section>

      <section>
        <h2>Data & Resources</h2>
        <p>
          All experiments rely on the shared prompt embedding dictionary
          (<code>prompt_embeds_dict.pth</code>) to avoid reloading the heavy T5
          encoder. The canonical test asset is <code>campanile.jpg</code>, while
          <code>dz.png</code> and <code>Sea images.png</code> provide additional
          content for SDEdit. Stage‚ÄØ1 runs at 64‚ÄØ√ó‚ÄØ64, Stage‚ÄØ2 upsamples to
          256‚ÄØ√ó‚ÄØ256, both in FP16 for memory efficiency.
        </p>
      </section>

      <section>
        <h2>What‚Äôs Done</h2>
        <div class="timeline">
          <div class="timeline-item">
            <div class="bubble">Part 0</div>
            <div>
              <strong>Model bring-up & prompt sampling</strong>
              <ul>
                <li>Authenticated to Hugging Face and loaded both DeepFloyd stages in FP16.</li>
                <li>Generated each provided prompt twice (short/long inference schedules) and compared fidelity.</li>
                <li>Recorded the global seed (180) to reuse across every subsequent cell.</li>
              </ul>
            </div>
          </div>
          <div class="timeline-item">
            <div class="bubble">Part 1</div>
            <div>
              <strong>Forward / reverse diffusion study</strong>
              <ul>
                <li>Implemented the analytical forward process and visualized noise growth at t‚ÄØ=‚ÄØ250/500/750.</li>
                <li>Benchmarked Gaussian blur vs. stage‚Äë1 UNet denoising to highlight classical vs. learned behavior.</li>
                <li>Built a strided timetable (990 ‚Üí 0) and iterative denoiser capable of single-step and multi-step reconstructions.</li>
              </ul>
            </div>
          </div>
          <div class="timeline-item">
            <div class="bubble">Part 2</div>
            <div>
              <strong>Sampling, CFG, & SDEdit</strong>
              <ul>
                <li>Sampled five random seeds via <code>iterative_denoise</code> to validate unconditional generation.</li>
                <li>Added classifier-free guidance with an empty prompt branch to boost detail at Œ≥‚ÄØ=‚ÄØ7.</li>
                <li>Applied SDEdit to the campanile and two personal photos at noise levels {1,3,5,7,10,20}.</li>
              </ul>
            </div>
          </div>
        </div>
      </section>

      <section>
        <h2>Key Takeaways</h2>
        <ul>
          <li>
            Small inference-step changes in Part‚ÄØ0 already alter texture sharpness, underscoring the compute/quality trade-off.
          </li>
          <li>
            Gaussian filtering removes high-frequency noise but destroys structure, while the UNet reconstruction keeps global geometry even at t‚ÄØ=‚ÄØ750.
          </li>
          <li>
            CFG produces noticeably richer lighting and edges at Œ≥‚ÄØ=‚ÄØ7, albeit with reduced diversity, matching the expected theory.
          </li>
          <li>
            SDEdit lets us dial in the edit strength: low i_start keeps the original subject, high i_start induces drastic hallucinations.
          </li>
        </ul>
      </section>

      <section>
        <h2>Detailed Outputs</h2>
        <p>
          Explore the full notebook renders or the condensed PDF-style summaries
          below. Both exports include inline figures for every experiment.
        </p>
        <div class="links">
          <a href="Project_4.html">üìò Full HTML Report</a>
          <a href="Project_4_Summary.html">üìù Executive Summary</a>
          <a href="Project_4 (3).py">üíª Cleaned Notebook Script</a>
        </div>
      </section>
    </div>
  </body>
</html>
