<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Diffusion Models · Project 4 Summary</title>
    <style>
      :root {
        --bg: #f8f8f6;
        --ink: #1d1f23;
        --muted: #5b616e;
        --line: #e3e3dd;
        --accent: #0055aa;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family: "IBM Plex Sans", system-ui, -apple-system, BlinkMacSystemFont,
          "Segoe UI", sans-serif;
        background: var(--bg);
        color: var(--ink);
        line-height: 1.6;
        padding: 40px 20px 64px;
      }

      .page {
        max-width: 1100px;
        margin: 0 auto;
        background: #fff;
        border: 1px solid var(--line);
        border-radius: 20px;
        padding: 48px clamp(24px, 5vw, 72px);
        box-shadow: 0 12px 32px rgba(0, 0, 0, 0.05);
      }

      h1 {
        font-size: clamp(2rem, 4vw, 3rem);
        letter-spacing: -0.02em;
      }

      header p {
        margin-top: 12px;
        color: var(--muted);
      }

      section {
        margin-top: 48px;
      }

      h2 {
        font-size: 1.3rem;
        letter-spacing: 0.02em;
        margin-bottom: 12px;
      }

      .summary {
        border-left: 3px solid var(--accent);
        padding-left: 18px;
        color: var(--muted);
      }

      .grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
        gap: 16px;
        margin-top: 20px;
      }

      .stat {
        border: 1px solid var(--line);
        border-radius: 14px;
        padding: 18px;
      }

      .stat strong {
        font-size: 2rem;
        color: var(--accent);
        display: block;
      }

      ul {
        list-style: none;
        color: var(--muted);
      }

      ul li {
        padding: 10px 0;
        border-bottom: 1px solid var(--line);
      }

      .timeline {
        border-left: 2px solid var(--line);
        margin-top: 12px;
      }

      .stage {
        margin-left: 24px;
        padding: 16px 0;
      }

      .stage h3 {
        font-size: 1rem;
        margin-bottom: 6px;
      }

      .gallery {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
        gap: 16px;
      }

      .gallery figure {
        border: 1px solid var(--line);
        border-radius: 16px;
        overflow: hidden;
        background: #fafafa;
      }

      .gallery img {
        width: 100%;
        display: block;
      }

      .gallery figcaption {
        padding: 10px 14px;
        font-size: 0.9rem;
        color: var(--muted);
      }

      .links {
        display: flex;
        flex-wrap: wrap;
        gap: 12px;
        margin-top: 16px;
      }

      .links a {
        text-decoration: none;
        color: var(--accent);
        border: 1px solid var(--accent);
        padding: 10px 16px;
        border-radius: 999px;
        font-weight: 500;
      }
    </style>
  </head>
  <body>
    <main class="page">
      <header>
        <h1>Diffusion Models · Project 4 Summary</h1>
        <p>
          Forward diffusion, classical filtering, neural denoisers, classifier‑free guidance, and
          SDEdit experiments executed with DeepFloyd IF. All image experiments share
          <code>torch.manual_seed(180)</code> for reproducibility.
        </p>
      </header>

      <section>
        <h2>Quick numbers</h2>
        <div class="grid">
          <div class="stat">
            <strong>3</strong>
            base prompts rendered in Part 0 using two inference-step budgets plus commentary.
          </div>
          <div class="stat">
            <strong>5</strong>
            CFG samples at γ = 7 comparing conditioned vs. unconditional noise predictions.
          </div>
          <div class="stat">
            <strong>6</strong>
            SDEdit starting indices (1 … 20) per image to illustrate edit strength.
          </div>
          <div class="stat">
            <strong>2</strong>
            custom assets (campanile, sea) reused across denoising and editing loops.
          </div>
        </div>
      </section>

      <section>
        <h2>Data & resources</h2>
        <p class="summary">
          Model weights: DeepFloyd IF stage 1/2 (FP16). Prompt embeddings cached in
          <code>prompt_embeds_dict.pth</code> to skip loading the large T5 encoder. Test imagery:
          <code>campanile.jpg</code> plus personal <code>dz.png</code> and <code>Sea images.png</code>.
        </p>
        <p style="margin-top:18px;font-size:0.95rem;color:var(--muted);">
          Reference artefacts: full HTML export (<code>Project_4.html</code>), condensed write-up
          (<code>Project_4_Summary.html</code>), and cleaned notebook script
          (<code>Project_4 (3).py</code>).
        </p>
      </section>

      <section>
        <h2>Work completed</h2>
        <div class="timeline">
          <div class="stage">
            <h3>Part 0 · model bring-up</h3>
            <ul>
              <li>Authenticated to Hugging Face, loaded both IF stages in FP16.</li>
              <li>Sampled three prompts at two inference-step counts and recorded observations.</li>
              <li>Captured the global seed (180) used everywhere else.</li>
            </ul>
          </div>
          <div class="stage">
            <h3>Part 1 · forward & reverse processes</h3>
            <ul>
              <li>Implemented the analytical forward process and visualized noise growth at t={250,500,750}.</li>
              <li>Compared Gaussian blur vs. stage‑1 UNet on the same noisy images.</li>
              <li>Created a 990→0 stride‑30 timetable and iterative denoiser (multi-step + single-step baselines).</li>
            </ul>
          </div>
          <div class="stage">
            <h3>Part 2 · sampling, CFG, SDEdit</h3>
            <ul>
              <li>Generated five unconditional samples via <code>iterative_denoise</code>.</li>
              <li>Added classifier-free guidance using empty prompt embeddings, γ = 7.</li>
              <li>Applied SDEdit to the campanile and two custom photos at starting indices {1,3,5,7,10,20}.</li>
            </ul>
          </div>
        </div>
      </section>

      <section>
        <h2>Visual evidence by part</h2>
        <p class="summary">
          Each figure references assets saved under <code>/images</code>. Captions emphasise the empirical
          observation that motivated the corresponding write-up in the report, giving the page a more
          academic narrative flow.
        </p>

        <h3 style="margin-top:24px">Part 0 · prompt sampling</h3>
        <p class="summary" style="margin-bottom:18px;">
          Objective: validate DeepFloyd IF with the supplied prompts, study the effect of
          <code>num_inference_steps</code>, and document qualitative differences prior to the analytical
          tasks.
        </p>
        <div class="gallery">
          <figure>
            <img src="images/image_000.png" alt="Stage 2 prompt comparison" />
            <figcaption>
              Stage 2 synthesis of the three provided prompts. Note how increasing <code>num_inference_steps</code>
              raises global sharpness yet adds diminishing detail beyond ~30 steps.
            </figcaption>
          </figure>
        </div>

        <h3 style="margin-top:28px">Part 1 · forward and reverse processes</h3>
        <p class="summary" style="margin-bottom:18px;">
          Objective: implement the closed-form forward process, evaluate classical Gaussian filtering,
          design an iterative denoiser, and compare it against single-step UNet inversion on the same
          noisy samples.
        </p>
        <div class="gallery">
          <figure>
            <img src="images/image_002.png" alt="Forward diffusion noise ladder" />
            <figcaption>
              Forward diffusion at t = 250/500/750 on campanile.jpg, visualising the scaling by
              <code>√ᾱ<sub>t</sub></code> and <code>√(1-ᾱ<sub>t</sub>)</code>.
            </figcaption>
          </figure>
          <figure>
            <img src="images/image_004.png" alt="Iterative vs single-step denoise" />
            <figcaption>
              Iterative denoise (stride 30) compared with single-step inversion and Gaussian blur,
              highlighting the benefit of gradual refinement.
            </figcaption>
          </figure>
        </div>

        <h3 style="margin-top:28px">Part 2 · sampling, CFG, SDEdit</h3>
        <p class="summary" style="margin-bottom:18px;">
          Objective: start from pure noise to produce unconditional samples, add classifier-free
          guidance to strengthen conditional paths, and apply SDEdit to quantify edit strength as a
          function of the starting index.
        </p>
        <div class="gallery">
          <figure>
            <img src="images/image_006.png" alt="CFG sampling at gamma 7" />
            <figcaption>
              CFG samples at γ = 7. The strengthened conditional branch yields higher contrast and
              semantic coherence relative to the unconditional baseline.
            </figcaption>
          </figure>
          <figure>
            <img src="images/image_007.png" alt="SDEdit progression" />
            <figcaption>
              SDEdit edits for the sea photograph. Larger starting indices induce increasingly
              creative hallucinations before converging back to the original manifold.
            </figcaption>
          </figure>
        </div>
      </section>
    </main>
  </body>
</html>
